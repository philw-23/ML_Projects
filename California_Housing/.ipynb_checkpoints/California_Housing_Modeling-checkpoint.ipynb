{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b6f8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "cores_to_leave = 2\n",
    "search = True # Boolean for if we run a random search\n",
    "run = False # Boolean for model determining runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59bd6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('./housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d31f61a",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae56cda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18659</th>\n",
       "      <td>-121.94</td>\n",
       "      <td>36.98</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3520.0</td>\n",
       "      <td>831.0</td>\n",
       "      <td>1486.0</td>\n",
       "      <td>753.0</td>\n",
       "      <td>3.0905</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16849</th>\n",
       "      <td>-122.42</td>\n",
       "      <td>37.62</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1355.0</td>\n",
       "      <td>214.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>6.3443</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5205</th>\n",
       "      <td>-118.29</td>\n",
       "      <td>33.94</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1782.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>2.5398</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13150</th>\n",
       "      <td>-121.47</td>\n",
       "      <td>36.92</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2049.0</td>\n",
       "      <td>417.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>4.6477</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>-122.15</td>\n",
       "      <td>37.69</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1246.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>637.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>3.6625</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "18659    -121.94     36.98                21.0       3520.0           831.0   \n",
       "16849    -122.42     37.62                39.0       1355.0           214.0   \n",
       "5205     -118.29     33.94                47.0       1782.0           338.0   \n",
       "13150    -121.47     36.92                27.0       2049.0           417.0   \n",
       "667      -122.15     37.69                38.0       1246.0           221.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \n",
       "18659      1486.0       753.0         3.0905      NEAR OCEAN  \n",
       "16849       682.0       246.0         6.3443      NEAR OCEAN  \n",
       "5205       1003.0       329.0         2.5398       <1H OCEAN  \n",
       "13150      1230.0       336.0         4.6477          INLAND  \n",
       "667         637.0       222.0         3.6625        NEAR BAY  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(housing.copy(), test_size=0.2) # Works on dataframes\n",
    "y_train = train['median_house_value'].values # Train targets\n",
    "y_test = test['median_house_value'].values # Test targets\n",
    "train.drop(['median_house_value'], axis=1, inplace=True) # Train: Leave only features\n",
    "test.drop(['median_house_value'], axis=1, inplace=True) # Test: Leave only features\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8bdaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an object for selecting specific variables of a DataFrame\n",
    "# Will be useful for building pipelines later as we can just input a dataframe\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Class MUST have a fit and transform function as this is what functions sklearn will look for in the pipeline\n",
    "# Thus we must watch the syntax\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attributes): # attributes represents a list of columns you want to select\n",
    "        self.attributes = attributes \n",
    "    # fit: This would be any transformations you want to perform (in this case we are just selecting columns)\n",
    "    # Note that you must include the y=None to match sklearn syntax\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    # transform: return selected columns\n",
    "    def transform(self, X): # X is a dataframe input\n",
    "        return X[self.attributes].values # Get numpy array of selected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "053e96f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step, we want to impute the missing categorical values\n",
    "# NOTE: must use train data ONLY to impute, don't want to touch test data\n",
    "from sklearn.impute import SimpleImputer\n",
    "numeric_cols = train.select_dtypes([np.number]).columns.tolist() # Get a list of numeric columns\n",
    "selector_num = DataFrameSelector(numeric_cols) # Object for selecting numeric columns\n",
    "train_impute = selector_num.transform(train) # Transform the data\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median') # Make the imputer - will use on test data later\n",
    "imputer.fit(train_impute) # Fit the data\n",
    "X_impute = imputer.transform(train_impute) # Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5b45dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "categorical_cols = train.select_dtypes(['object']).columns.tolist() # Categorical columns\n",
    "selector_cat = DataFrameSelector(categorical_cols) # Initialize selector\n",
    "train_onehot = selector_cat.transform(train) # Transform the data\n",
    "onehot = OneHotEncoder() # Generate OneHotEncoder object\n",
    "onehot.fit(train_onehot) # Fit\n",
    "X_onehot = onehot.transform(train_onehot) # Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38fc4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler on numerical data\n",
    "# Note: Don't include targets here!\n",
    "# Per: https://stackoverflow.com/questions/26584971/how-to-not-standarize-target-data-in-scikit-learn-regression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "x_scaler = StandardScaler() # initialize object\n",
    "x_scaler.fit(X_impute) # Fit imputed data\n",
    "X_scaled = x_scaler.transform(X_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba898e",
   "metadata": {},
   "source": [
    "# Pipeline Structure\n",
    "* I think all of the above can be done in a pipeline structure\n",
    "* Basically performing all these steps in a defined sequence\n",
    "* Good to document it out once though just for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68911c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Numerical feature pipe\n",
    "numerical_pipe = Pipeline([\n",
    "    ('num_select', DataFrameSelector(numeric_cols)),\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical feature pipe\n",
    "categorical_pipe = Pipeline([\n",
    "    ('cat_select', DataFrameSelector(categorical_cols)),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine into pre-processing pipeline single object\n",
    "prep_pipe = FeatureUnion([\n",
    "    ('numerical', numerical_pipe),\n",
    "    ('categorical', categorical_pipe)\n",
    "])\n",
    "\n",
    "# NOTE: Can access elements of a feature union for random/grid search per https://www.kaggle.com/edolatabadi/feature-union-with-grid-search\n",
    "# Can then run everything through these at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe11686",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "* Select sample models\n",
    "* Create a scoring object \n",
    "* Evaluate and see which models perform best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19b85e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to test\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "models = {'RandomForest':RandomForestRegressor(),\n",
    "         'AdaBoost':AdaBoostRegressor(),\n",
    "         'GradBoost':GradientBoostingRegressor()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccb4cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline on training data\n",
    "prep_pipe.fit(train, y_train)\n",
    "X_train = prep_pipe.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa52bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "run = False # Boolean for this cell\n",
    "\n",
    "# Function for printing out scores\n",
    "def print_scores(score_array):\n",
    "    print('Errors: ', score_array)\n",
    "    print('Mean Error: ', score_array.mean())\n",
    "    print('Std: ', score_array.std())\n",
    "    \n",
    "if run:\n",
    "    for k, model in models.items():\n",
    "        print('Model: ' + k)\n",
    "        m_scores = cross_val_score(model, X_train, y_train, # Have to flatten transformed y_data\n",
    "                                  scoring='neg_mean_squared_error', cv=10)\n",
    "        m_scores = np.sqrt(-m_scores) # Because using negative mean squared error\n",
    "        print_scores(m_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3fbca",
   "metadata": {},
   "source": [
    "# Randomized Search\n",
    "* RandomForest was the best performing\n",
    "* Let's use a randomized search to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da958f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 FeatureUnion(transformer_list=[('numerical',\n",
       "                                                 Pipeline(steps=[('num_select',\n",
       "                                                                  DataFrameSelector(attributes=['longitude',\n",
       "                                                                                                'latitude',\n",
       "                                                                                                'housing_median_age',\n",
       "                                                                                                'total_rooms',\n",
       "                                                                                                'total_bedrooms',\n",
       "                                                                                                'population',\n",
       "                                                                                                'households',\n",
       "                                                                                                'median_income'])),\n",
       "                                                                 ('imputer',\n",
       "                                                                  SimpleImputer()),\n",
       "                                                                 ('scaler',\n",
       "                                                                  StandardScaler())])),\n",
       "                                                ('categorical',\n",
       "                                                 Pipeline(steps=[('cat_select',\n",
       "                                                                  DataFrameSelector(attributes=['ocean_proximity'])),\n",
       "                                                                 ('onehot',\n",
       "                                                                  OneHotEncoder())]))])),\n",
       "                ('reg', RandomForestRegressor())])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# Need to make a model pipeline\n",
    "full_pipe = Pipeline([\n",
    "    ('prep', prep_pipe),\n",
    "    ('reg', RandomForestRegressor())\n",
    "])\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3e1661b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-b603421ed971>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Run Search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msearcher\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_pipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcores_to_leave\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0msearch_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[0;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set param grid\n",
    "param_grid = {\n",
    "    'prep__numerical__scaler':[StandardScaler(), MinMaxScaler()],\n",
    "    'reg__bootstrap':[True, False],\n",
    "    'reg__n_estimators':[10, 50, 100, 200, 300],\n",
    "    'reg__max_features':[2, 4, 6, 8, 10]\n",
    "}\n",
    "\n",
    "if search:\n",
    "    # Run Search\n",
    "    searcher = GridSearchCV(full_pipe, param_grid, cv=20, n_jobs=3)\n",
    "    search_res = searcher.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d370e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are our best parameters?\n",
    "print('Best Params: ', search_res.best_params_)\n",
    "all_results = pd.DataFrame.from_dict(search_res.cv_results_) # Dataframe of all results\n",
    "all_results.to_csv('./grid_search_results')\n",
    "print(all_results.head())\n",
    "\n",
    "# Set best model params\n",
    "# Note: could also use search_res.best_estimator_ to get the best performing model\n",
    "full_pipe = full_pipe.set_params(**search_res.best_params_) # Set pipeline with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b67c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's do another cross val with out best model\n",
    "scores = cross_val_score(full_pipe, train, y_train, # Have to flatten transformed y_data\n",
    "                          scoring='neg_mean_squared_error', cv=10)\n",
    "scores = np.sqrt(-scores)\n",
    "print_scores(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
