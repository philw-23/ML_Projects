{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1377629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "cores_to_leave = 2\n",
    "search = True # Boolean for if we run a random search\n",
    "run = False # Boolean for model determining runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb12fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('./housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e8a24d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8336648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>-118.29</td>\n",
       "      <td>34.08</td>\n",
       "      <td>34.0</td>\n",
       "      <td>479.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.5250</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>-122.20</td>\n",
       "      <td>37.79</td>\n",
       "      <td>49.0</td>\n",
       "      <td>882.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>737.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>2.6667</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20564</th>\n",
       "      <td>-121.71</td>\n",
       "      <td>38.72</td>\n",
       "      <td>32.0</td>\n",
       "      <td>710.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>2.8882</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>-118.34</td>\n",
       "      <td>33.98</td>\n",
       "      <td>47.0</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>2374.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>2.3882</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13253</th>\n",
       "      <td>-117.68</td>\n",
       "      <td>34.11</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3190.0</td>\n",
       "      <td>471.0</td>\n",
       "      <td>1414.0</td>\n",
       "      <td>464.0</td>\n",
       "      <td>5.5292</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "4339     -118.29     34.08                34.0        479.0           182.0   \n",
       "236      -122.20     37.79                49.0        882.0           195.0   \n",
       "20564    -121.71     38.72                32.0        710.0           155.0   \n",
       "8369     -118.34     33.98                47.0       2649.0           684.0   \n",
       "13253    -117.68     34.11                16.0       3190.0           471.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \n",
       "4339        557.0       170.0         1.5250       <1H OCEAN  \n",
       "236         737.0       210.0         2.6667        NEAR BAY  \n",
       "20564       550.0       154.0         2.8882          INLAND  \n",
       "8369       2374.0       607.0         2.3882       <1H OCEAN  \n",
       "13253      1414.0       464.0         5.5292          INLAND  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(housing.copy(), test_size=0.2) # Works on dataframes\n",
    "y_train = train['median_house_value'].values # Train targets\n",
    "y_test = test['median_house_value'].values # Test targets\n",
    "train.drop(['median_house_value'], axis=1, inplace=True) # Train: Leave only features\n",
    "test.drop(['median_house_value'], axis=1, inplace=True) # Test: Leave only features\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0abe3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an object for selecting specific variables of a DataFrame\n",
    "# Will be useful for building pipelines later as we can just input a dataframe\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Class MUST have a fit and transform function as this is what functions sklearn will look for in the pipeline\n",
    "# Thus we must watch the syntax\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attributes): # attributes represents a list of columns you want to select\n",
    "        self.attributes = attributes \n",
    "    # fit: This would be any transformations you want to perform (in this case we are just selecting columns)\n",
    "    # Note that you must include the y=None to match sklearn syntax\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    # transform: return selected columns\n",
    "    def transform(self, X): # X is a dataframe input\n",
    "        return X[self.attributes] # Return selected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49f7dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another function for adding specific attributes\n",
    "# Currently just replacing Longitude, Lattitude w/ Spherical Coordinates\n",
    "class GeoReplacer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X): # X is a dataframe input\n",
    "        X['sphere_x'] = np.cos(np.radians(X['latitude'])) * np.cos(np.radians(X['longitude']))\n",
    "        X['sphere_y'] = np.cos(np.radians(X['latitude'])) * np.sin(np.radians(X['longitude']))\n",
    "        X['sphere_z'] = np.sin(np.radians(X['latitude']))\n",
    "        X.drop(columns=['latitude', 'longitude'], axis=1, inplace=True) # Drop these as they are unwanted\n",
    "        return X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18996b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step, we want to impute the missing categorical values\n",
    "# NOTE: must use train data ONLY to impute, don't want to touch test data\n",
    "from sklearn.impute import SimpleImputer\n",
    "numeric_cols = train.select_dtypes([np.number]).columns.tolist() # Get a list of numeric columns\n",
    "selector_num = DataFrameSelector(numeric_cols) # Object for selecting numeric columns\n",
    "train_impute = selector_num.transform(train) # Transform the data\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median') # Make the imputer - will use on test data later\n",
    "imputer.fit(train_impute) # Fit the data\n",
    "X_impute = imputer.transform(train_impute) # Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96355140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "categorical_cols = train.select_dtypes(['object']).columns.tolist() # Categorical columns\n",
    "selector_cat = DataFrameSelector(categorical_cols) # Initialize selector\n",
    "train_onehot = selector_cat.transform(train) # Transform the data\n",
    "onehot = OneHotEncoder() # Generate OneHotEncoder object\n",
    "onehot.fit(train_onehot) # Fit\n",
    "X_onehot = onehot.transform(train_onehot) # Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0bd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler on numerical data\n",
    "# Note: Don't include targets here!\n",
    "# Per: https://stackoverflow.com/questions/26584971/how-to-not-standarize-target-data-in-scikit-learn-regression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "x_scaler = StandardScaler() # initialize object\n",
    "x_scaler.fit(X_impute) # Fit imputed data\n",
    "X_scaled = x_scaler.transform(X_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f30f04",
   "metadata": {},
   "source": [
    "# Pipeline Structure\n",
    "* I think all of the above can be done in a pipeline structure\n",
    "* Basically performing all these steps in a defined sequence\n",
    "* Good to document it out once though just for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4af06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Numerical feature pipe\n",
    "numerical_pipe = Pipeline([\n",
    "    ('num_select', DataFrameSelector(numeric_cols)), # SELECT COLUMNS FIRST\n",
    "    ('geo_replace', GeoReplacer()), # THEN TRANSFORM\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical feature pipe\n",
    "categorical_pipe = Pipeline([\n",
    "    ('cat_select', DataFrameSelector(categorical_cols)),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine into pre-processing pipeline single object\n",
    "prep_pipe = FeatureUnion([\n",
    "    ('numerical', numerical_pipe),\n",
    "    ('categorical', categorical_pipe)\n",
    "])\n",
    "\n",
    "# NOTE: Can access elements of a feature union for random/grid search per https://www.kaggle.com/edolatabadi/feature-union-with-grid-search\n",
    "# Can then run everything through these at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfb1999",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "* Select sample models\n",
    "* Create a scoring object \n",
    "* Evaluate and see which models perform best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292e9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to test\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "models = {'RandomForest':RandomForestRegressor(),\n",
    "         'AdaBoost':AdaBoostRegressor(),\n",
    "         'GradBoost':GradientBoostingRegressor()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425455dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline on training data\n",
    "prep_pipe.fit(train, y_train)\n",
    "X_train = prep_pipe.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e3b92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "run = False # Boolean for this cell\n",
    "\n",
    "# Function for printing out scores\n",
    "def print_scores(score_array):\n",
    "    print('Errors: ', score_array)\n",
    "    print('Mean Error: ', score_array.mean())\n",
    "    print('Std: ', score_array.std())\n",
    "    \n",
    "if run:\n",
    "    for k, model in models.items():\n",
    "        print('Model: ' + k)\n",
    "        m_scores = cross_val_score(model, X_train, y_train, # Have to flatten transformed y_data\n",
    "                                  scoring='neg_mean_squared_error', cv=10)\n",
    "        m_scores = np.sqrt(-m_scores) # Because using negative mean squared error\n",
    "        print_scores(m_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3146715",
   "metadata": {},
   "source": [
    "# Randomized Search\n",
    "* RandomForest was the best performing\n",
    "* Let's use a randomized search to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f1059d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 FeatureUnion(transformer_list=[('numerical',\n",
       "                                                 Pipeline(steps=[('num_select',\n",
       "                                                                  DataFrameSelector(attributes=['longitude',\n",
       "                                                                                                'latitude',\n",
       "                                                                                                'housing_median_age',\n",
       "                                                                                                'total_rooms',\n",
       "                                                                                                'total_bedrooms',\n",
       "                                                                                                'population',\n",
       "                                                                                                'households',\n",
       "                                                                                                'median_income'])),\n",
       "                                                                 ('geo_replace',\n",
       "                                                                  GeoReplacer()),\n",
       "                                                                 ('imputer',\n",
       "                                                                  SimpleImputer()),\n",
       "                                                                 ('scaler',\n",
       "                                                                  StandardScaler())])),\n",
       "                                                ('categorical',\n",
       "                                                 Pipeline(steps=[('cat_select',\n",
       "                                                                  DataFrameSelector(attributes=['ocean_proximity'])),\n",
       "                                                                 ('onehot',\n",
       "                                                                  OneHotEncoder())]))])),\n",
       "                ('reg', RandomForestRegressor())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# Need to make a model pipeline\n",
    "full_pipe = Pipeline([\n",
    "    ('prep', prep_pipe),\n",
    "    ('reg', RandomForestRegressor())\n",
    "])\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2a771dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set param grid\n",
    "param_grid = {\n",
    "    'prep__numerical__scaler':[StandardScaler(), MinMaxScaler()],\n",
    "    'reg__bootstrap':[True, False],\n",
    "    'reg__n_estimators':[10, 50, 100, 200, 300],\n",
    "    'reg__max_features':[2, 4, 6, 8, 10]\n",
    "}\n",
    "\n",
    "if search:\n",
    "    # Run Search\n",
    "    searcher = GridSearchCV(full_pipe, param_grid, n_jobs=5) # Default 5 fold for grid search\n",
    "    search_res = searcher.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8e00177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'prep__numerical__scaler': MinMaxScaler(), 'reg__bootstrap': False, 'reg__max_features': 4, 'reg__n_estimators': 200}\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       1.263146      0.026284         0.014213        0.000400   \n",
      "1       6.239221      0.054300         0.045441        0.002061   \n",
      "2      11.565435      0.212375         0.081074        0.004821   \n",
      "3      23.900326      0.195370         0.151738        0.002061   \n",
      "4      34.106656      0.296401         0.223003        0.002318   \n",
      "\n",
      "  param_prep__numerical__scaler param_reg__bootstrap param_reg__max_features  \\\n",
      "0              StandardScaler()                 True                       2   \n",
      "1              StandardScaler()                 True                       2   \n",
      "2              StandardScaler()                 True                       2   \n",
      "3              StandardScaler()                 True                       2   \n",
      "4              StandardScaler()                 True                       2   \n",
      "\n",
      "  param_reg__n_estimators                                             params  \\\n",
      "0                      10  {'prep__numerical__scaler': StandardScaler(), ...   \n",
      "1                      50  {'prep__numerical__scaler': StandardScaler(), ...   \n",
      "2                     100  {'prep__numerical__scaler': StandardScaler(), ...   \n",
      "3                     200  {'prep__numerical__scaler': StandardScaler(), ...   \n",
      "4                     300  {'prep__numerical__scaler': StandardScaler(), ...   \n",
      "\n",
      "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0           0.785787           0.786715           0.800466           0.799317   \n",
      "1           0.825589           0.813963           0.827557           0.820359   \n",
      "2           0.824819           0.819873           0.830843           0.825195   \n",
      "3           0.826895           0.820469           0.832668           0.827971   \n",
      "4           0.828386           0.821389           0.833174           0.828930   \n",
      "\n",
      "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0           0.794194         0.793296        0.006135              100  \n",
      "1           0.821020         0.821698        0.004723               82  \n",
      "2           0.822319         0.824610        0.003659               76  \n",
      "3           0.823672         0.826335        0.004114               69  \n",
      "4           0.822887         0.826953        0.004293               66  \n"
     ]
    }
   ],
   "source": [
    "# What are our best parameters?\n",
    "print('Best Params: ', search_res.best_params_)\n",
    "all_results = pd.DataFrame.from_dict(search_res.cv_results_) # Dataframe of all results\n",
    "all_results.to_csv('./grid_search_results')\n",
    "print(all_results.head())\n",
    "\n",
    "# Set best model params\n",
    "# Note: could also use search_res.best_estimator_ to get the best performing model\n",
    "full_pipe = full_pipe.set_params(**search_res.best_params_) # Set pipeline with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a93bcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors:  [46375.86781174 44543.90994171 47143.05195741 48231.20909617\n",
      " 44763.89551764 44519.99821026 45656.16170849 44802.01564379\n",
      " 48489.10531814 43696.20205432]\n",
      "Mean Error:  45822.141725967434\n",
      "Std:  1581.2932099460072\n"
     ]
    }
   ],
   "source": [
    "# Now, let's do another cross val with out best model\n",
    "scores = cross_val_score(full_pipe, train, y_train, # Have to flatten transformed y_data\n",
    "                          scoring='neg_mean_squared_error', cv=10)\n",
    "scores = np.sqrt(-scores)\n",
    "print_scores(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
