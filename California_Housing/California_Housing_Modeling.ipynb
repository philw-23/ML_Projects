{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c3a401a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "cores_to_leave = 2\n",
    "search = True # Boolean for if we run a random search\n",
    "run = False # Boolean for model determining runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ac4fcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv('./housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196dd4e0",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "561bf301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6167</th>\n",
       "      <td>-117.93</td>\n",
       "      <td>34.09</td>\n",
       "      <td>35.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>4.2062</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8421</th>\n",
       "      <td>-118.35</td>\n",
       "      <td>33.93</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3156.0</td>\n",
       "      <td>857.0</td>\n",
       "      <td>2394.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>3.0100</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10790</th>\n",
       "      <td>-117.91</td>\n",
       "      <td>33.63</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1122.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>3.5388</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>-121.59</td>\n",
       "      <td>39.78</td>\n",
       "      <td>18.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>2.1838</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8786</th>\n",
       "      <td>-118.34</td>\n",
       "      <td>33.78</td>\n",
       "      <td>25.0</td>\n",
       "      <td>11016.0</td>\n",
       "      <td>1626.0</td>\n",
       "      <td>4168.0</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>8.1782</td>\n",
       "      <td>NEAR OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "6167     -117.93     34.09                35.0        782.0           153.0   \n",
       "8421     -118.35     33.93                26.0       3156.0           857.0   \n",
       "10790    -117.91     33.63                32.0       1122.0           233.0   \n",
       "1114     -121.59     39.78                18.0        945.0           205.0   \n",
       "8786     -118.34     33.78                25.0      11016.0          1626.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \n",
       "6167        499.0       163.0         4.2062       <1H OCEAN  \n",
       "8421       2394.0       787.0         3.0100       <1H OCEAN  \n",
       "10790       557.0       223.0         3.5388       <1H OCEAN  \n",
       "1114        385.0       207.0         2.1838          INLAND  \n",
       "8786       4168.0      1584.0         8.1782      NEAR OCEAN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into test and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(housing.copy(), test_size=0.2) # Works on dataframes\n",
    "y_train = train['median_house_value'].values # Train targets\n",
    "y_test = test['median_house_value'].values # Test targets\n",
    "train.drop(['median_house_value'], axis=1, inplace=True) # Train: Leave only features\n",
    "test.drop(['median_house_value'], axis=1, inplace=True) # Test: Leave only features\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65275fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an object for selecting specific variables of a DataFrame\n",
    "# Will be useful for building pipelines later as we can just input a dataframe\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Class MUST have a fit and transform function as this is what functions sklearn will look for in the pipeline\n",
    "# Thus we must watch the syntax\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attributes): # attributes represents a list of columns you want to select\n",
    "        self.attributes = attributes \n",
    "    # fit: This would be any transformations you want to perform (in this case we are just selecting columns)\n",
    "    # Note that you must include the y=None to match sklearn syntax\n",
    "    def fit(self, X, y=None): \n",
    "        return self\n",
    "    # transform: return selected columns\n",
    "    def transform(self, X): # X is a dataframe input\n",
    "        return X[self.attributes].values # Get numpy array of selected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0e03f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step, we want to impute the missing categorical values\n",
    "# NOTE: must use train data ONLY to impute, don't want to touch test data\n",
    "from sklearn.impute import SimpleImputer\n",
    "numeric_cols = train.select_dtypes([np.number]).columns.tolist() # Get a list of numeric columns\n",
    "selector_num = DataFrameSelector(numeric_cols) # Object for selecting numeric columns\n",
    "train_impute = selector_num.transform(train) # Transform the data\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median') # Make the imputer - will use on test data later\n",
    "imputer.fit(train_impute) # Fit the data\n",
    "X_impute = imputer.transform(train_impute) # Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4669a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode the categorical data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "categorical_cols = train.select_dtypes(['object']).columns.tolist() # Categorical columns\n",
    "selector_cat = DataFrameSelector(categorical_cols) # Initialize selector\n",
    "train_onehot = selector_cat.transform(train) # Transform the data\n",
    "onehot = OneHotEncoder() # Generate OneHotEncoder object\n",
    "onehot.fit(train_onehot) # Fit\n",
    "X_onehot = onehot.transform(train_onehot) # Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8159ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler on numerical data\n",
    "# Note: Don't include targets here!\n",
    "# Per: https://stackoverflow.com/questions/26584971/how-to-not-standarize-target-data-in-scikit-learn-regression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "x_scaler = StandardScaler() # initialize object\n",
    "x_scaler.fit(X_impute) # Fit imputed data\n",
    "X_scaled = x_scaler.transform(X_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84142e",
   "metadata": {},
   "source": [
    "# Pipeline Structure\n",
    "* I think all of the above can be done in a pipeline structure\n",
    "* Basically performing all these steps in a defined sequence\n",
    "* Good to document it out once though just for practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a7f2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Numerical feature pipe\n",
    "numerical_pipe = Pipeline([\n",
    "    ('num_select', DataFrameSelector(numeric_cols)),\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical feature pipe\n",
    "categorical_pipe = Pipeline([\n",
    "    ('cat_select', DataFrameSelector(categorical_cols)),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Combine into pre-processing pipeline single object\n",
    "prep_pipe = FeatureUnion([\n",
    "    ('numerical', numerical_pipe),\n",
    "    ('categorical', categorical_pipe)\n",
    "])\n",
    "\n",
    "# NOTE: Can access elements of a feature union for random/grid search per https://www.kaggle.com/edolatabadi/feature-union-with-grid-search\n",
    "# Can then run everything through these at once!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323917c7",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "* Select sample models\n",
    "* Create a scoring object \n",
    "* Evaluate and see which models perform best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97108cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to test\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "models = {'RandomForest':RandomForestRegressor(),\n",
    "         'AdaBoost':AdaBoostRegressor(),\n",
    "         'GradBoost':GradientBoostingRegressor()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16c11db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline on training data\n",
    "prep_pipe.fit(train, y_train)\n",
    "X_train = prep_pipe.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ebdbc67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "run = False # Boolean for this cell\n",
    "\n",
    "# Function for printing out scores\n",
    "def print_scores(score_array):\n",
    "    print('Errors: ', score_array)\n",
    "    print('Mean Error: ', score_array.mean())\n",
    "    print('Std: ', score_array.std())\n",
    "    \n",
    "if run:\n",
    "    for k, model in models.items():\n",
    "        print('Model: ' + k)\n",
    "        m_scores = cross_val_score(model, X_train, y_train, # Have to flatten transformed y_data\n",
    "                                  scoring='neg_mean_squared_error', cv=10)\n",
    "        m_scores = np.sqrt(-m_scores) # Because using negative mean squared error\n",
    "        print_scores(m_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83aaac",
   "metadata": {},
   "source": [
    "# Randomized Search\n",
    "* RandomForest was the best performing\n",
    "* Let's use a randomized search to find best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a21e38d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 FeatureUnion(transformer_list=[('numerical',\n",
       "                                                 Pipeline(steps=[('num_select',\n",
       "                                                                  DataFrameSelector(attributes=['longitude',\n",
       "                                                                                                'latitude',\n",
       "                                                                                                'housing_median_age',\n",
       "                                                                                                'total_rooms',\n",
       "                                                                                                'total_bedrooms',\n",
       "                                                                                                'population',\n",
       "                                                                                                'households',\n",
       "                                                                                                'median_income'])),\n",
       "                                                                 ('imputer',\n",
       "                                                                  SimpleImputer()),\n",
       "                                                                 ('scaler',\n",
       "                                                                  StandardScaler())])),\n",
       "                                                ('categorical',\n",
       "                                                 Pipeline(steps=[('cat_select',\n",
       "                                                                  DataFrameSelector(attributes=['ocean_proximity'])),\n",
       "                                                                 ('onehot',\n",
       "                                                                  OneHotEncoder())]))])),\n",
       "                ('reg', RandomForestRegressor())])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# Need to make a model pipeline\n",
    "full_pipe = Pipeline([\n",
    "    ('prep', prep_pipe),\n",
    "    ('reg', RandomForestRegressor())\n",
    "])\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb132bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set param grid\n",
    "param_grid = {\n",
    "    'prep__numerical__scaler':[StandardScaler(), MinMaxScaler()],\n",
    "    'reg__bootstrap':[True, False],\n",
    "    'reg__n_estimators':[10, 50, 100, 200, 300],\n",
    "    'reg__max_features':[2, 4, 6, 8, 10]\n",
    "}\n",
    "\n",
    "if search:\n",
    "    # Run Search\n",
    "    searcher = GridSearchCV(full_pipe, param_grid, cv=20, n_jobs=4)\n",
    "    search_res = searcher.fit(train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c32748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are our best parameters?\n",
    "print('Best Params: ', search_res.best_params_)\n",
    "all_results = pd.DataFrame.from_dict(search_res.cv_results_) # Dataframe of all results\n",
    "all_results.to_csv('./grid_search_results')\n",
    "print(all_results.head())\n",
    "\n",
    "# Set best model params\n",
    "# Note: could also use search_res.best_estimator_ to get the best performing model\n",
    "full_pipe = full_pipe.set_params(**search_res.best_params_) # Set pipeline with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb8cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's do another cross val with out best model\n",
    "scores = cross_val_score(full_pipe, train, y_train, # Have to flatten transformed y_data\n",
    "                          scoring='neg_mean_squared_error', cv=10)\n",
    "scores = np.sqrt(-scores)\n",
    "print_scores(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
